{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "A 2 layer MLP is sufficient to model most functions. Why are deep networks used instead of 2 layer networks? \n",
    "\n",
    "   Your answer: Because it is only sufficient under the condition that the width of the neural network can be arbitrarily big (Universal Approximation Theorem), which is not feasible for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "What are the differences between stochastic gradient descent and Batch gradient descent? Why is mini-batch gradient descent used in practice?\n",
    "\n",
    "   Your answer: Batch gradient descent is deterministic, requires iterating over the entire training sample, and is generally slow. Stochastic gradient descent is stochastic, uses one one data point from the sample to estimate the gradient and is generally quick. The noisiness of SGD also helps it escape local minima. Mini-batch combines the speed associated with SGD with higher accuracy from taking an average of multiple data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why are activation functions used in deep learning? What are the issues that can arise when using sigmoid or tanh activation functions?\n",
    "\n",
    "   Your answer: Activation functions introduce a non-linearity, which allows ANNs to model non-linear functions. Otherise, an ANN could only model a linear function. Sigmoid and tanh both have vanishing gradiants due to them exibiting asymptotic behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What are the differences between linear and logistic regression? Are both linear?\n",
    "\n",
    "   Your answer: Linear regression fits a hyperplane to a set of data and logisitc regression fits a logisitc curve in order to classify (usually) binary data. Linear classifiers are linear while the logistic curve does not meet the criterion to be considered linear as a function. However, as a linear classifier, ie it seperates data in a way equivalent to using a hyperplane, it IS linear.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: The best model I had was actually pretty standard and got an accuracy of 83%. I perhaps got really lucky with the RNG on weights, so the default parameters got me 93% when I added more epochs. If that didn't work I might have tried using momentum / increased hidden layer size.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: The motivation behind a my perplexity was that anything much higher didn't give clustering and anything too low also had too much overlap "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
